{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f305ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25103298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:17:02\tHassan:\tmorning, sir\r\n",
      "00:18:02\tHaidar Rahman (Edo):\tDeals with Big Data\r\n",
      "00:18:13\tHabib Abdurrasyid:\tnetworks that inspired by human neuron and how it works\r\n",
      "00:18:14\tUtih Amartiwi:\tlike a blackbox\r\n",
      "00:18:40\tMutiara Ruci:\tann with 2 layer. deep learning have more layer?\r\n",
      "00:18:56\tWayan Dadang - Frontera Orbit Class:\tnonlinier model\r\n",
      "00:24:26\tUtih Amartiwi:\tyes\r\n",
      "00:24:28\tHaidar Rahman (Edo):\tyes\r\n",
      "00:24:29\tPulung Hendro Prastyo:\tclear\r\n",
      "00:24:33\tFarchan Hakim:\tclear sir\r\n",
      "00:24:36\tWayan Dadang - Frontera Orbit Class:\tclear\r\n",
      "00:24:38\tHassan:\tyes, sir\r\n",
      "00:31:04\tHabib Abdurrasyid:\tIn data science domain, is deep learning rarely used sir?\r\n",
      "00:31:52\tHabib Abdurrasyid:\tI see, thank you sir\r\n",
      "00:34:08\totm3:\tare there any good implementation of AI for image recognition which not using DL sir?\r\n",
      "00:34:21\tDino Febriyanto:\tsir, talk about mirror. with GAN we can \"mirroring\" someone face, right?\r\n",
      "00:34:52\tDino Febriyanto:\tahh i see\r\n",
      "00:34:57\tDino Febriyanto:\tthanks sir\r\n",
      "00:41:24\tHabib Abdurrasyid:\tOr maybe mining rig üòÅ\r\n",
      "00:41:56\tDino Febriyanto:\tbut we can't playing game on cloud system hahah\r\n",
      "00:42:34\tDino Febriyanto:\tXd\r\n",
      "00:42:36\tDino Febriyanto:\tXD\r\n",
      "00:43:16\tDino Febriyanto:\tclear sir\r\n",
      "00:43:17\tHabib Abdurrasyid:\tClear\r\n",
      "00:43:21\tWayan Dadang - Frontera Orbit Class:\tGPU = kidney\r\n",
      "00:43:22\tA_Mahaputra Ilham Awal:\tclear sir\r\n",
      "00:49:16\tUtih Amartiwi:\tclear\r\n",
      "00:56:02\tUtih Amartiwi:\tclear\r\n",
      "00:59:15\totm3:\tI don't sir\r\n",
      "00:59:19\tFendy Hendriyanto:\tautomatic Code\r\n",
      "00:59:21\tMahaputra Ilham Awal:\tHelp me typing code\r\n",
      "00:59:21\tMutiara Ruci:\tyes sir\r\n",
      "00:59:25\tWahyuni Rahmawati:\thelp programmer to code\r\n",
      "00:59:30\tHabib Abdurrasyid:\tProgrammer pleasure to code bunch of code\r\n",
      "00:59:44\tFendy Hendriyanto:\tLike a Tabnine for typing code\r\n",
      "01:00:10\tWayan Dadang - Frontera Orbit Class:\tTabnine vs GitHub copilot\r\n",
      "01:03:10\tFauzan Firdaus:\tsir, as we know, one of the most important thing in deep learning/neural network is activation function. what do you think about that? and how do we find and use the best activation function for certain AI systems?\r\n",
      "01:03:42\tFauzan Firdaus:\tthankyou sir\r\n",
      "01:03:48\tWayan Dadang - Frontera Orbit Class:\tSir, can you exlain about Amdhal's Law in paraller Computing like GPU?\r\n",
      "01:04:00\tWayan Dadang - Frontera Orbit Class:\t*parallel\r\n",
      "01:04:35\tFaisal Asadi:\tSir, do we talk too about CNN also ?\r",
      "Thank you\r\n",
      "01:06:08\totm3:\tD\r\n",
      "01:06:09\tFaisal Asadi:\tD\r\n",
      "01:06:09\tWayan Dadang - Frontera Orbit Class:\tD\r\n",
      "01:06:10\tHassan:\tD\r\n",
      "01:06:10\tHabib Abdurrasyid:\tD\r\n",
      "01:06:11\tMuhamad Ikhsan:\td\r\n",
      "01:06:11\tMukhlizar Nirwan:\tD\r\n",
      "01:06:12\tMahaputra Ilham Awal:\tD\r\n",
      "01:06:13\tMochammad Andrian Maulana:\tD\r\n",
      "01:06:13\tFauzan Firdaus:\tD\r\n",
      "01:06:33\tYevonnael Andrew:\tSir, does it mean by the advent of CNN and deep learning for images, the classic statistical images processing is obsolete? If no, in what case the classical can be more advantageous than deep learning?\r\n",
      "01:07:24\tYevonnael Andrew:\tyes sir\r\n",
      "01:08:26\tYevonnael Andrew:\tok thanks sir\r\n",
      "01:10:30\tFauzan Firdaus:\tfrom the feature itself\r\n",
      "01:10:42\tFaisal Asadi:\tFeature extraction\r\n",
      "01:16:54\totm3:\tFor initial weight and bias is it better with constant accross all neuron or random across all neuron sir?\r\n",
      "01:17:52\tMutiara Ruci:\tbias is always 1 sir?\r\n",
      "01:18:16\tMutiara Ruci:\tokay thank you sir\r\n",
      "01:24:06\tDino Febriyanto:\tis it convert the problem in to non-linear problem, sir?\r\n",
      "01:24:45\tDino Febriyanto:\toke sir\r\n",
      "01:24:46\tDino Febriyanto:\tthanks\r\n",
      "01:29:04\tHabib Abdurrasyid:\t1/(1+ e^(-160.5)) = 1\r\n",
      "01:32:13\tHabib Abdurrasyid:\tClear sir\r\n",
      "01:32:15\tUtih Amartiwi:\tclear for me\r\n",
      "01:32:16\tMutiara Ruci:\twhat problem we use tanh sir?\r\n",
      "01:32:20\tWayan Dadang - Frontera Orbit Class:\tclear sir\r\n",
      "01:32:37\tFaisal Asadi:\twhen we use tan H activ function?\r\n",
      "01:32:56\tMutiara Ruci:\tso it same like sigmoid sir?\r\n",
      "01:32:59\tDino Febriyanto:\twhen we use sigmoid ot tanh?\r\n",
      "01:33:02\tDino Febriyanto:\t*or\r\n",
      "01:33:05\tHabib Abdurrasyid:\twhen to use sigmoid and tanh?\r\n",
      "01:33:13\tYusup Hidayat:\tso to find the best activation function, we only need to look at the output we want?\r\n",
      "01:33:32\tFaisal Asadi:\tcan we use than H more than 2 classes?\r\n",
      "01:33:55\tFaisal Asadi:\tI see, so we use SoftMax. Thank you\r\n",
      "01:34:31\tDino Febriyanto:\tclear sir\r\n",
      "01:35:26\totm3:\texcuse me sir for hidden layer what activation function we use?\r\n",
      "01:35:50\tMutiara Ruci:\trelu\r\n",
      "01:37:45\tMutiara Ruci:\tso we use relu for avoid - value sir?\r\n",
      "01:38:02\tDino Febriyanto:\tyes\r\n",
      "01:38:16\tDino Febriyanto:\tavoid minus value\r\n",
      "01:38:17\tMutiara Ruci:\tnegative\r\n",
      "01:39:53\tYusup Hidayat:\tzero\r\n",
      "01:39:55\tMahaputra Ilham Awal:\tzero\r\n",
      "01:39:56\tHabib Abdurrasyid:\t0\r\n",
      "01:39:57\tDino Febriyanto:\t0\r\n",
      "01:40:06\tFaisal Asadi:\tshould be 0\r\n",
      "01:41:16\tFio faberio:\tclear\r\n",
      "01:41:16\totm3:\tIs there any use of non linear activation function like logarithmic or exponential function in hidden layer?\r\n",
      "01:42:02\totm3:\tOk sir\r\n",
      "01:47:28\totm3:\tis it possible that gradient descent to find and settle to local minima but miss the global minima?\r\n",
      "01:50:41\totm3:\thow gradient calculated if it's taken at a time in stochastics sir? I think gradient calculated as (after-before)\r\n",
      "01:50:53\tDino Febriyanto - Kelas Taihu:\twhen we have to use batch, stochastic, and mini-batch, sir?\r\n",
      "01:51:44\tMuhammad Angga Muttaqien:\tSir, mathematically, can we know does it reach the global optima? Or at least can we know how close is it?\r\n",
      "01:53:16\tDino Febriyanto - Kelas Taihu:\tclear sir\r\n",
      "01:53:28\tHabib Abdurrasyid:\tC\r\n",
      "01:53:28\totm3:\tc\r\n",
      "01:53:29\tDino Febriyanto - Kelas Taihu:\tC\r\n",
      "01:53:31\tEly Sudarsono:\tC\r\n",
      "01:53:31\tMahaputra Ilham Awal:\tC\r\n",
      "01:53:31\tUtih Amartiwi:\tC\r\n",
      "01:53:32\tFendy Hendriyanto:\tC\r\n",
      "01:53:33\tWayan Dadang - Frontera Orbit Class:\tSofmax\r\n",
      "01:53:35\tFaisal Asadi:\tc\r\n",
      "01:56:24\tHabib Abdurrasyid:\tclear sir\r\n",
      "01:56:27\tFaisal Asadi:\tclear\r\n",
      "01:56:35\tWayan Dadang - Frontera Orbit Class:\tclaar\r\n",
      "01:56:35\tUtih Amartiwi:\tclear\r\n",
      "02:12:16\totm3:\tback propagation is a gradient descent to update the weight in training ANN sir?\r\n",
      "02:12:43\totm3:\tOk sir, clear my doubt.\r\n",
      "02:13:31\tMukhlizar Nirwan:\twhat is difference back propagation and gradient descent sir ?\r\n",
      "02:14:05\tMukhlizar Nirwan:\tokaay sir\r\n",
      "02:14:07\tMutiara Ruci:\tgot it sir. but just in case, when we feed forward we dont update weight sir? we do update when backprogation after we get error\r\n",
      "02:14:18\tYusup Hidayat:\thow long will gradient descent keep iterating?\r\n",
      "02:14:48\tFaisal Asadi:\tSir why BPNN giving more best accuracy if compared to LVQ?\r\n",
      "02:16:09\tFaisal Asadi:\tok thank sir\r\n",
      "02:16:25\tMuhammad Angga Muttaqien:\tSir, according to the explanation, since leaky ReLu tries to solve the problem present on ReLu, so can we say leaky ReLu is always better?\r\n",
      "02:16:53\tMuhammad Angga Muttaqien:\tOK sir\r\n",
      "02:17:08\tMuhammad Angga Muttaqien:\tSir, do you think it is better to explain about vanishing & exploding gradients first to the students before explaining about activation function?\r\n",
      "02:18:16\tMuhammad Angga Muttaqien:\tOK sir thanks\r\n",
      "02:25:29\tFaisal Asadi:\tis it not giving bias value for accuracy if we do early stopping?\r\n",
      "02:31:59\tYusup Hidayat:\tyour voice is not heard sir\r\n",
      "02:32:09\tMutiara Ruci:\tim sorry sir i cant hear the voic3\r\n",
      "02:34:59\tFaisal Asadi:\tis there any strategy/algorithm to find best dropout value that can increasing accuracy?\r\n",
      "02:35:14\tHabib Abdurrasyid:\tD\r\n",
      "02:35:14\tFaisal Asadi:\tD\r\n",
      "02:35:18\tMahaputra Ilham Awal:\tD\r\n",
      "02:35:21\tUtih Amartiwi:\tD\r\n",
      "02:35:22\tFauzan Firdaus:\tD\r\n",
      "02:35:46\tMuhammad Rifal Alfarizy:\tExcuse me sir, is the dropout always placed on the fully connected layer? can the dropout be placed outside the fully connected layer sir? For example, it is placed after the pooling layer\r\n",
      "02:36:17\tMuhammad Rifal Alfarizy:\tThanks sir\r\n",
      "02:37:06\tYusup Hidayat:\tcan you explain about the optimizer sir? like Adam and Nadam.\r\n",
      "02:40:31\tYusup Hidayat:\tthank you sir\r\n",
      "02:41:11\tFaisal Asadi:\tcould you give the simulation how to give dropout value sir?\r\n",
      "02:41:49\tFaisal Asadi:\tOK, thank you Sir\r\n",
      "02:42:39\tYusup Hidayat:\tfine sir\r\n",
      "03:43:20\tUtih Amartiwi:\tyes\r\n",
      "03:43:26\tDino Febriyanto:\tyes\r\n",
      "04:00:16\tHabib Abdurrasyid:\twhy don't we use softmax for output layer instead of sigmoid sir?\r\n",
      "04:03:02\totm3:\tExcuse me sir, can you repeat the cross entropy?\r\n",
      "04:03:43\totm3:\tOk sir.\r\n",
      "04:05:48\tDino Febriyanto - Kelas Taihu:\tsir, why we use sigmoid instead of SoftMax?\r\n",
      "04:14:04\tDino Febriyanto:\tuse argmax\r\n",
      "04:17:10\tDino Febriyanto:\tclear sir\r\n",
      "04:17:36\tHabib Abdurrasyid:\tIn this case the SoftMax give higher accuracy, right?\r\n",
      "04:18:08\tHabib Abdurrasyid:\tGot it, thank you sir\r\n",
      "04:19:44\tHabib Abdurrasyid:\tCan we use grid search like in sklearn sir? üòÅ\r\n",
      "04:19:52\totm3:\tis there any deterministic approach to determine the hyper parameter sir?\r\n",
      "04:22:18\tHabib Abdurrasyid:\tOk, thank you sir\r\n",
      "04:22:24\tHabib Abdurrasyid:\tClear\r\n",
      "04:22:35\tDino Febriyanto:\tclear sir\r\n",
      "04:24:13\tHabib Abdurrasyid:\tSir can you please explain about transfer learning?\r\n",
      "04:24:26\tHabib Abdurrasyid:\tOk sir\r\n",
      "04:24:58\tDino Febriyanto:\tsure\r\n",
      "04:24:58\totm3:\tYes\r\n",
      "04:46:13\tUtih Amartiwi:\tclear\r\n",
      "04:46:24\tWayan Dadang - Frontera Orbit Class:\tclear\r\n",
      "04:46:28\tDino Febriyanto - Kelas Taihu:\tall clear sir\r\n",
      "04:46:48\tYusup Hidayat:\tis there any special case we use sequential model or function base model?\r\n",
      "04:53:58\tYusup Hidayat:\tclear, thank you sir\r\n",
      "05:01:44\tDino Febriyanto - Kelas Taihu:\tsir why the loss is so huge?\r\n",
      "05:03:26\tDino Febriyanto - Kelas Taihu:\tclear sir\r\n",
      "05:03:44\tDino Febriyanto - Kelas Taihu:\tso for regression problem we use linear func actiovation?\r\n",
      "05:04:17\tDino Febriyanto - Kelas Taihu:\tclear sir\r\n",
      "05:04:21\tDino Febriyanto - Kelas Taihu:\tthank you\r\n",
      "05:04:21\tUtih Amartiwi:\tclear\r\n",
      "05:04:53\tWayan Dadang - Frontera Orbit Class:\tclear\r\n",
      "05:05:00\tDino Febriyanto - Kelas Taihu:\tclear sir\r\n",
      "05:05:41\tYusup Hidayat:\tclear sir\r\n",
      "05:07:44\tAman Ulla:\thttps://gofile.io/d/X66K9L\r\n",
      "05:07:59\tWayan Dadang - Frontera Orbit Class:\tThank you, sir\r\n",
      "05:08:24\tUtih Amartiwi:\tthank you\r\n",
      "05:08:35\tHassan:\tthank you, sir.\r\n",
      "05:08:50\tDino Febriyanto - Kelas Taihu:\tno sir\r\n",
      "05:08:52\tDino Febriyanto - Kelas Taihu:\tthank you\r\n",
      "05:09:20\tWayan Dadang - Frontera Orbit Class:\tok\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = codecs.open('example1.txt', 'r', 'utf-8')\n",
    "text = f.readlines()\n",
    "text = ''.join(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47794fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = re.findall(\"\\d{2}:\\d{2}:\\d{2}\", text)\n",
    "messages = re.split(\"\\d{2}:\\d{2}:\\d{2}\", text)\n",
    "messages.pop(0)\n",
    "users = []\n",
    "comments = []\n",
    "for message in messages:\n",
    "    users.append(re.findall(\"\\t([\\S\\s]+):\", message)[0])\n",
    "    comments.append(re.findall(\":\\t([\\S\\s]+)\", message)[0].strip())\n",
    "\n",
    "new_timestamps = []\n",
    "for t in timestamps:\n",
    "    second = int(t[-2:]) + 2\n",
    "    new_timestamps.append(f\"{t},000 --> {t[:-2]}{second},000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c27c17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('result.srt', 'w+', 'utf-8') as f:\n",
    "    for i in range(len(users)):\n",
    "        f.write(f\"{str(i+1)}\\n\")\n",
    "        f.write(f\"{new_timestamps[i]}\\n\")\n",
    "        f.write(f\"{users[i]}: {comments[i]}\\n\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71378591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-xplore] *",
   "language": "python",
   "name": "conda-env-ml-xplore-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
